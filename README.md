# ğŸ§  Credit Card Default Prediction from Natural Language Queries

Predict credit card default risk directly from plain English using LLMs + deep tabular learning.

> Example input:  
> â€œA 28-year-old man with a limit of 20,000 and late on his last 4 payments. Bills are rising and he pays only the minimum.â€

ğŸ”— <video src="credit_default_demo.mp4" controls="controls" width="100%" />

## ğŸš€ Overview

This project turns a sentence into a risk score using:
- ğŸ§  **Claude 3 (Anthropic)**: Parses free-text into structured features
- ğŸ”„ **Feature Engineering**: Binning, WOE, ratios, and aggregations
- âš–ï¸ **SMOTE**: For class imbalance
- ğŸ¯ **Model Training**: Logistic Regression, Random Forest, XGBoost, Meta Learners, FT-Transformer
- ğŸ“ˆ **Hyperparameter Tuning**: GridSearchCV + Optuna
- ğŸ§ª **Cross-Validation**: Robust metrics (AUC, precision, recall, F1)
- ğŸ’» **Gradio UI**: Predict defaults from natural language, instantly

---

## ğŸ“Š Dataset

Based on the [UCI Credit Card Default dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients):

- ~23 features including age, payment history, bill amounts, and credit limits
- Highly imbalanced target (default vs non-default)

---

## ğŸ”§ Feature Engineering

âœ… Log-transformations (e.g., `LIMIT_BAL_log`)  
âœ… Binning + Weight of Evidence (WOE) encoding  
âœ… Aggregations: average bill/payment, ratios (e.g., `PAY_AMT_RATIO`)  
âœ… Delta calculations (e.g., `PAY_DIFF` = avg bill - avg payment)

---

## ğŸ§ª Models Implemented

| Model                         | Preprocessing        | Tuning       | CV Supported | Notes                        |
|------------------------------|----------------------|--------------|--------------|------------------------------|
| Logistic Regression          | One-Hot + Scaling    | GridSearchCV | âœ…           | Baseline + WOE variant       |
| Random Forest                | One-Hot              | GridSearchCV | âœ…           | Higher recall                |
| XGBoost                      | One-Hot              | GridSearchCV | âœ…           | Good AUC, fast inference     |
| Meta Learner (Stacked)       | Ensemble             | Manual       | âœ…           | Base: LR + RF + XGB          |
| Calibrated Meta Learner      | Ensemble + Prob Cal. | Manual       | âœ…           | Improves confidence outputs  |
| **FT-Transformer**           | One-Hot + Scaling    | Optuna       | âœ…           | Deep tabular model           |
| **FT-Transformer (Deep)**    | + Dropout, 100 Epochs| Optuna       | âœ…           | Best performer               |

---

## ğŸ“‰ Performance Summary

### ğŸ”¹ FT-Transformer (SMOTE + Deep + Optuna)
- AUC: **0.8631**
- Accuracy: **77.9%**
- Precision: **77.6%**
- Recall: **78.4%**
- F1 Score: **0.780**
- Confusion Matrix:  
  - TP = 2422  
  - TN = 2390  
  - FP = 699  
  - FN = 666

### ğŸ”¹ TabTransformer (Baseline)
- AUC: **0.7608**
- Accuracy: **80.5%**
- Precision: **62.1%**
- Recall: **37.8%**
- F1 Score: **0.470**

---

## âš ï¸ Error Sensitivity: FP vs FN

- **False Positive (FP)**: Model flags someone as risky who isnâ€™t â†’ loss of good customer.
- **False Negative (FN)**: Model misses a defaulter â†’ financial loss to lender.

> A strong model balances high precision (fewer FPs) and high recall (fewer FNs).

---

## ğŸ’¬ Gradio Interface (LLM-Powered)

- ğŸ§  Claude extracts structured features from plain English
- ğŸ¯ Passes to trained model (FT-Transformer)
- ğŸ“Š Returns predicted risk and confidence

```python
# Example query
"A 45-year-old woman, credit limit 30,000, delayed past 3 payments, pays ~500 monthly."

â†’ Output: Default Probability: 72.4%
```


ğŸ“‚ Project Structure

ğŸ“ CreditCardDefaulters
```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Credit_Card_train.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ft_transformer_model.h5
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda_feature_engineering.ipynb
â”‚   â”œâ”€â”€ modeling_gridsearch_optuna.ipynb
â”œâ”€â”€ app.py           # Gradio Interface
â”œâ”€â”€ utils.py         # Claude API call + feature extractor
â””â”€â”€ README.md
```



